#!/usr/bin/env python3

import json
import uuid
import os
import argh
import shell as sh
import aws

def add_step(terminate_on_fail, cluster_id, name, *args):
    resp = aws.client('emr').add_job_flow_steps(
        JobFlowId=cluster_id,
        Steps=[{'Name': name,
                'ActionOnFailure': 'TERMINATE_CLUSTER' if terminate_on_fail else 'CANCEL_AND_WAIT',
                'HadoopJarStep': {'Jar': 'command-runner.jar',
                                  'Args': args}}]
    )
    print(json.dumps(resp, indent=4))

def main(cluster_id, script_file, terminate_on_fail=False, interactive=False):
    if interactive:
        sh.check_call(f'aws-emr-scp {script_file} :/tmp/script.hql {cluster_id} 1>&2')
        sh.check_call(f'aws-emr-ssh {cluster_id} --cmd "time hive -f /tmp/script.hql"')
    else:
        bucket = os.environ['AWS_EMR_SCRIPT_BUCKET']
        script_path = f's3://{bucket}/tmp/scripts/{uuid.uuid4()}'
        sh.check_call('aws s3 cp', script_file, script_path)
        add_step(terminate_on_fail, cluster_id, 'copy script', 'aws', 's3', 'cp', script_path, '/tmp/script.hql')
        add_step(terminate_on_fail, cluster_id, 'run script', 'hive', '-f', '/tmp/script.hql')

if __name__ == '__main__':
    with aws.setup():
        argh.dispatch_command(main)
