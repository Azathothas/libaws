#!/usr/bin/env python3
# type: ignore
import json
import uuid
import boto3
import os
import argh
import shell
import cli_aws

def add_step(terminate_on_fail, cluster_id, name, *args):
    resp = boto3.client('emr').add_job_flow_steps(
        JobFlowId=cluster_id,
        Steps=[{'Name': name,
                'ActionOnFailure': 'TERMINATE_CLUSTER' if terminate_on_fail else 'CANCEL_AND_WAIT',
                'HadoopJarStep': {'Jar': 'command-runner.jar',
                                  'Args': args}}]
    )

    print(json.dumps(resp, indent=4))

def main(cluster_id, schema_file, script_file, terminate_on_fail=False):
    bucket = os.environ['AWS_EMR_SCRIPT_BUCKET']
    schema_path = f's3://{bucket}/tmp/scripts/{uuid.uuid4()}'
    script_path = f's3://{bucket}/tmp/scripts/{uuid.uuid4()}'
    shell.run('aws s3 cp', schema_file, schema_path)
    shell.run('aws s3 cp', script_file, script_path)
    add_step(terminate_on_fail, cluster_id, 'copy schema', 'aws', 's3', 'cp', schema_path, '/tmp/schema.hql')
    add_step(terminate_on_fail, cluster_id, 'copy script', 'aws', 's3', 'cp', script_path, '/tmp/script.hql')
    add_step(terminate_on_fail, cluster_id, 'run script', 'hive', '-i', '/tmp/schema.hql', '-f', '/tmp/script.hql')

if __name__ == '__main__':
    with cli_aws.setup():
        argh.dispatch_command(main)
