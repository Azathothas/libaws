#!/usr/bin/env python3
# type: ignore
import json
import uuid
import boto3
import os
import argh
import shell
import cli_aws

def add_step(terminate_on_fail, cluster_id, name, *args):
    resp = boto3.client('emr').add_job_flow_steps(
        JobFlowId=cluster_id,
        Steps=[{'Name': name,
                'ActionOnFailure': 'TERMINATE_CLUSTER' if terminate_on_fail else 'CANCEL_AND_WAIT',
                'HadoopJarStep': {'Jar': 'command-runner.jar',
                                  'Args': args}}]
    )
    print(json.dumps(resp, indent=4))

def main(cluster_id, schema_file, script_file, terminate_on_fail=False, interactive=False):
    if interactive:
        shell.check_call(f'aws-emr-scp {schema_file} :/tmp/schema.hql {cluster_id} 1>&2')
        shell.check_call(f'aws-emr-scp {script_file} :/tmp/script.hql {cluster_id} 1>&2')
        shell.check_call(f'aws-emr-ssh {cluster_id} --cmd "hive -i /tmp/schema.hql -f /tmp/script.hql"')
    else:
        bucket = os.environ['AWS_EMR_SCRIPT_BUCKET']
        schema_path = f's3://{bucket}/tmp/scripts/{uuid.uuid4()}'
        script_path = f's3://{bucket}/tmp/scripts/{uuid.uuid4()}'
        shell.check_call('aws s3 cp', schema_file, schema_path)
        shell.check_call('aws s3 cp', script_file, script_path)
        add_step(terminate_on_fail, cluster_id, 'copy schema', 'aws', 's3', 'cp', schema_path, '/tmp/schema.hql')
        add_step(terminate_on_fail, cluster_id, 'copy script', 'aws', 's3', 'cp', script_path, '/tmp/script.hql')
        add_step(terminate_on_fail, cluster_id, 'run script', 'hive', '-i', '/tmp/schema.hql', '-f', '/tmp/script.hql')

if __name__ == '__main__':
    with cli_aws.setup():
        argh.dispatch_command(main)
